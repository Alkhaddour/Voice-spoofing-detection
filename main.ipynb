{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from RNN_trainer import train\n",
    "from datasets import ReplySpoofDataset, collate_fn_pad\n",
    "from config import LSTM_NUM_LAYERS, HIDDEN_SIZE, BATCH_SIZE, INPUT_SIZE, LINEAR_SIZE, OUTPUT_SIZE, LR, N_EPOCHS, \\\n",
    "    TRAIN_INDEX, VAL_INDEX, MODELS_DIR, MODEL_NAME, OUTPUT_DIR, TEST_RAW_DIR, SCALER_PATH, SCHEDULER_STEP_SIZE, \\\n",
    "    SCHEDULER_GAMMA\n",
    "from utilities.basic_utils import make_valid_path, get_accelerator, export_incorrect_samples_to_csv\n",
    "from utilities.model_utils import ModelManager, Metrics\n",
    "from utilities.disply_utils import plot_losses, info\n",
    "from RNN_tester import test, test_sample\n",
    "from models import AntiSpoofingRNN\n",
    "import IPython.display as ipd\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2bf4596fc70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-11-04 01:24:27.866504] -- Loading data (40001 files)...\n",
      "[2021-11-04 01:26:29.873367] -- Done...\n",
      "[2021-11-04 01:26:29.882920] -- Loading data (9999 files)...\n",
      "[2021-11-04 01:27:32.765530] -- Done...\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset= ReplySpoofDataset(TRAIN_INDEX)\n",
    "val_dataset = ReplySpoofDataset(VAL_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = AntiSpoofingRNN(INPUT_SIZE, HIDDEN_SIZE, LSTM_NUM_LAYERS, LINEAR_SIZE, OUTPUT_SIZE)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_pad)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-11-04 01:27:32.805064] -- Training\n",
      "[2021-11-04 01:27:36.186132] -- Current lr = [0.0005]\n",
      "[2021-11-04 01:28:22.282983] -- Training:   batch 150/626 from epoch 01/60 -- Loss = 0.49967899918556213\n",
      "[2021-11-04 01:28:53.240267] -- Training:   batch 300/626 from epoch 01/60 -- Loss = 0.3753916919231415\n",
      "[2021-11-04 01:29:24.569645] -- Training:   batch 450/626 from epoch 01/60 -- Loss = 0.46179118752479553\n",
      "[2021-11-04 01:29:55.905288] -- Training:   batch 600/626 from epoch 01/60 -- Loss = 0.5969678163528442\n",
      "[2021-11-04 01:30:01.115374] -- Training:   batch 626/626 from epoch 01/60 -- Loss = 0.20117299258708954\n",
      "[2021-11-04 01:30:04.802873] -- Validating: batch 050/157 from epoch 01/60 -- Loss = 0.5535688400268555\n",
      "[2021-11-04 01:30:08.451172] -- Validating: batch 100/157 from epoch 01/60 -- Loss = 0.5063414573669434\n",
      "[2021-11-04 01:30:12.121405] -- Validating: batch 150/157 from epoch 01/60 -- Loss = 0.5063385963439941\n",
      "[2021-11-04 01:30:12.596545] -- Validating: batch 157/157 from epoch 01/60 -- Loss = 0.5016041994094849\n",
      "[2021-11-04 01:30:12.599537] -- Epoch 01/60 -- Train loss = 0.5114371488793209, Validation loss = 0.5112722119328322 \n",
      "[2021-11-04 01:30:12.614616] -- Model updated, new loss 0.5112722119328322\n",
      "[2021-11-04 01:30:12.614616] -- Current lr = [0.0005]\n",
      "[2021-11-04 01:30:43.462107] -- Training:   batch 150/626 from epoch 02/60 -- Loss = 0.7067683339118958\n",
      "[2021-11-04 01:31:14.478326] -- Training:   batch 300/626 from epoch 02/60 -- Loss = 0.525416910648346\n",
      "[2021-11-04 01:31:45.412619] -- Training:   batch 450/626 from epoch 02/60 -- Loss = 0.5270380973815918\n",
      "[2021-11-04 01:32:16.429935] -- Training:   batch 600/626 from epoch 02/60 -- Loss = 0.39051637053489685\n",
      "[2021-11-04 01:32:21.658875] -- Training:   batch 626/626 from epoch 02/60 -- Loss = 0.19816379249095917\n",
      "[2021-11-04 01:32:25.269685] -- Validating: batch 050/157 from epoch 02/60 -- Loss = 0.4351985454559326\n",
      "[2021-11-04 01:32:28.903834] -- Validating: batch 100/157 from epoch 02/60 -- Loss = 0.36376774311065674\n",
      "[2021-11-04 01:32:32.557319] -- Validating: batch 150/157 from epoch 02/60 -- Loss = 0.4828255772590637\n",
      "[2021-11-04 01:32:33.054975] -- Validating: batch 157/157 from epoch 02/60 -- Loss = 0.5018420219421387\n",
      "[2021-11-04 01:32:33.054975] -- Epoch 02/60 -- Train loss = 0.5095851426354994, Validation loss = 0.5116099048951629 \n",
      "[2021-11-04 01:32:33.054975] -- Best loss 0.5112722119328322\n",
      "[2021-11-04 01:32:33.054975] -- Current lr = [0.0005]\n",
      "[2021-11-04 01:33:03.980005] -- Training:   batch 150/626 from epoch 03/60 -- Loss = 0.48475804924964905\n",
      "[2021-11-04 01:33:34.912564] -- Training:   batch 300/626 from epoch 03/60 -- Loss = 0.4519999623298645\n",
      "[2021-11-04 01:34:05.943756] -- Training:   batch 450/626 from epoch 03/60 -- Loss = 0.41593480110168457\n",
      "[2021-11-04 01:34:37.150184] -- Training:   batch 600/626 from epoch 03/60 -- Loss = 0.46315997838974\n",
      "[2021-11-04 01:34:42.431257] -- Training:   batch 626/626 from epoch 03/60 -- Loss = 1.5663859844207764\n",
      "[2021-11-04 01:34:46.116033] -- Validating: batch 050/157 from epoch 03/60 -- Loss = 0.4657791256904602\n",
      "[2021-11-04 01:34:49.781935] -- Validating: batch 100/157 from epoch 03/60 -- Loss = 0.6245545744895935\n",
      "[2021-11-04 01:34:53.421716] -- Validating: batch 150/157 from epoch 03/60 -- Loss = 0.5054740905761719\n",
      "[2021-11-04 01:34:53.916559] -- Validating: batch 157/157 from epoch 03/60 -- Loss = 0.5014992952346802\n",
      "[2021-11-04 01:34:53.917556] -- Epoch 03/60 -- Train loss = 0.5117004289032933, Validation loss = 0.5096198532991348 \n",
      "[2021-11-04 01:34:53.921546] -- Model updated, new loss 0.5096198532991348\n",
      "[2021-11-04 01:34:53.921546] -- Current lr = [0.0005]\n",
      "[2021-11-04 01:35:24.865901] -- Training:   batch 150/626 from epoch 04/60 -- Loss = 0.5048363208770752\n",
      "[2021-11-04 01:35:55.937577] -- Training:   batch 300/626 from epoch 04/60 -- Loss = 0.6014950275421143\n",
      "[2021-11-04 01:36:26.937545] -- Training:   batch 450/626 from epoch 04/60 -- Loss = 0.602686882019043\n",
      "[2021-11-04 01:36:57.927672] -- Training:   batch 600/626 from epoch 04/60 -- Loss = 0.5751373767852783\n",
      "[2021-11-04 01:37:03.213964] -- Training:   batch 626/626 from epoch 04/60 -- Loss = 0.24568019807338715\n",
      "[2021-11-04 01:37:06.927607] -- Validating: batch 050/157 from epoch 04/60 -- Loss = 0.6058056950569153\n",
      "[2021-11-04 01:37:10.588135] -- Validating: batch 100/157 from epoch 04/60 -- Loss = 0.5052215456962585\n",
      "[2021-11-04 01:37:14.230000] -- Validating: batch 150/157 from epoch 04/60 -- Loss = 0.4448433816432953\n",
      "[2021-11-04 01:37:14.719845] -- Validating: batch 157/157 from epoch 04/60 -- Loss = 0.41532230377197266\n",
      "[2021-11-04 01:37:14.720843] -- Epoch 04/60 -- Train loss = 0.5093792349623796, Validation loss = 0.509000833247118 \n",
      "[2021-11-04 01:37:14.725885] -- Model updated, new loss 0.509000833247118\n",
      "[2021-11-04 01:37:14.725946] -- Current lr = [0.0005]\n",
      "[2021-11-04 01:37:45.647275] -- Training:   batch 150/626 from epoch 05/60 -- Loss = 0.5491936206817627\n",
      "[2021-11-04 01:38:16.665781] -- Training:   batch 300/626 from epoch 05/60 -- Loss = 0.5473864078521729\n",
      "[2021-11-04 01:38:47.720321] -- Training:   batch 450/626 from epoch 05/60 -- Loss = 0.5675127506256104\n",
      "[2021-11-04 01:39:18.611075] -- Training:   batch 600/626 from epoch 05/60 -- Loss = 0.5500193238258362\n",
      "[2021-11-04 01:39:23.825640] -- Training:   batch 626/626 from epoch 05/60 -- Loss = 0.23641343414783478\n",
      "[2021-11-04 01:39:27.427400] -- Validating: batch 050/157 from epoch 05/60 -- Loss = 0.5885878801345825\n",
      "[2021-11-04 01:39:31.055317] -- Validating: batch 100/157 from epoch 05/60 -- Loss = 0.5256943702697754\n",
      "[2021-11-04 01:39:34.706462] -- Validating: batch 150/157 from epoch 05/60 -- Loss = 0.3999463617801666\n",
      "[2021-11-04 01:39:35.182488] -- Validating: batch 157/157 from epoch 05/60 -- Loss = 0.32169032096862793\n",
      "[2021-11-04 01:39:35.182488] -- Epoch 05/60 -- Train loss = 0.5092652568373436, Validation loss = 0.5082761037881207 \n",
      "[2021-11-04 01:39:35.186477] -- Model updated, new loss 0.5082761037881207\n",
      "[2021-11-04 01:39:35.186477] -- Current lr = [0.0005]\n",
      "[2021-11-04 01:40:05.959815] -- Training:   batch 150/626 from epoch 06/60 -- Loss = 0.5481259822845459\n",
      "[2021-11-04 01:40:36.919338] -- Training:   batch 300/626 from epoch 06/60 -- Loss = 0.546082079410553\n",
      "[2021-11-04 01:41:07.970948] -- Training:   batch 450/626 from epoch 06/60 -- Loss = 0.44579970836639404\n",
      "[2021-11-04 01:41:38.711199] -- Training:   batch 600/626 from epoch 06/60 -- Loss = 0.48292866349220276\n",
      "[2021-11-04 01:41:43.983487] -- Training:   batch 626/626 from epoch 06/60 -- Loss = 0.24523892998695374\n",
      "[2021-11-04 01:41:47.677445] -- Validating: batch 050/157 from epoch 06/60 -- Loss = 0.46474793553352356\n",
      "[2021-11-04 01:41:51.329677] -- Validating: batch 100/157 from epoch 06/60 -- Loss = 0.5859472751617432\n",
      "[2021-11-04 01:41:54.958142] -- Validating: batch 150/157 from epoch 06/60 -- Loss = 0.44453293085098267\n",
      "[2021-11-04 01:41:55.448071] -- Validating: batch 157/157 from epoch 06/60 -- Loss = 0.5010209083557129\n",
      "[2021-11-04 01:41:55.449063] -- Epoch 06/60 -- Train loss = 0.5093547430948708, Validation loss = 0.5093910379014956 \n",
      "[2021-11-04 01:41:55.449063] -- Best loss 0.5082761037881207\n",
      "[2021-11-04 01:41:55.449063] -- Current lr = [0.0005]\n",
      "[2021-11-04 01:42:26.265074] -- Training:   batch 150/626 from epoch 07/60 -- Loss = 0.5257812738418579\n",
      "[2021-11-04 01:42:58.295248] -- Training:   batch 300/626 from epoch 07/60 -- Loss = 0.4405462145805359\n",
      "[2021-11-04 01:43:29.365899] -- Training:   batch 450/626 from epoch 07/60 -- Loss = 0.5477616190910339\n",
      "[2021-11-04 01:44:00.439039] -- Training:   batch 600/626 from epoch 07/60 -- Loss = 0.4635380506515503\n",
      "[2021-11-04 01:44:05.663851] -- Training:   batch 626/626 from epoch 07/60 -- Loss = 1.6231261491775513\n",
      "[2021-11-04 01:44:09.321608] -- Validating: batch 050/157 from epoch 07/60 -- Loss = 0.39895355701446533\n",
      "[2021-11-04 01:44:13.041070] -- Validating: batch 100/157 from epoch 07/60 -- Loss = 0.42010897397994995\n",
      "[2021-11-04 01:44:16.749974] -- Validating: batch 150/157 from epoch 07/60 -- Loss = 0.5047081112861633\n",
      "[2021-11-04 01:44:17.248300] -- Validating: batch 157/157 from epoch 07/60 -- Loss = 0.4100888967514038\n",
      "[2021-11-04 01:44:17.248300] -- Epoch 07/60 -- Train loss = 0.5113398013785243, Validation loss = 0.5087139910193765 \n",
      "[2021-11-04 01:44:17.248300] -- Best loss 0.5082761037881207\n",
      "[2021-11-04 01:44:17.248300] -- Current lr = [0.0005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-11-04 01:44:48.504905] -- Training:   batch 150/626 from epoch 08/60 -- Loss = 0.44398587942123413\n",
      "[2021-11-04 01:45:20.491653] -- Training:   batch 300/626 from epoch 08/60 -- Loss = 0.6115612983703613\n",
      "[2021-11-04 01:45:51.830327] -- Training:   batch 450/626 from epoch 08/60 -- Loss = 0.5954432487487793\n",
      "[2021-11-04 01:46:23.386654] -- Training:   batch 600/626 from epoch 08/60 -- Loss = 0.5476207137107849\n",
      "[2021-11-04 01:46:28.854385] -- Training:   batch 626/626 from epoch 08/60 -- Loss = 0.23809656500816345\n",
      "[2021-11-04 01:46:32.537923] -- Validating: batch 050/157 from epoch 08/60 -- Loss = 0.5462130308151245\n",
      "[2021-11-04 01:46:36.195502] -- Validating: batch 100/157 from epoch 08/60 -- Loss = 0.5255399346351624\n",
      "[2021-11-04 01:46:39.860022] -- Validating: batch 150/157 from epoch 08/60 -- Loss = 0.670250415802002\n",
      "[2021-11-04 01:46:40.368643] -- Validating: batch 157/157 from epoch 08/60 -- Loss = 0.5007554292678833\n",
      "[2021-11-04 01:46:40.368643] -- Epoch 08/60 -- Train loss = 0.5094370734863007, Validation loss = 0.5091757466838618 \n",
      "[2021-11-04 01:46:40.369650] -- Best loss 0.5082761037881207\n",
      "[2021-11-04 01:46:40.369650] -- Current lr = [0.0005]\n",
      "[2021-11-04 01:47:11.727449] -- Training:   batch 150/626 from epoch 09/60 -- Loss = 0.5653684139251709\n",
      "[2021-11-04 01:47:43.258160] -- Training:   batch 300/626 from epoch 09/60 -- Loss = 0.41548770666122437\n",
      "[2021-11-04 01:48:14.548504] -- Training:   batch 450/626 from epoch 09/60 -- Loss = 0.6064751148223877\n",
      "[2021-11-04 01:48:46.437330] -- Training:   batch 600/626 from epoch 09/60 -- Loss = 0.5047112703323364\n",
      "[2021-11-04 01:48:51.875786] -- Training:   batch 626/626 from epoch 09/60 -- Loss = 1.6348309516906738\n",
      "[2021-11-04 01:48:55.595591] -- Validating: batch 050/157 from epoch 09/60 -- Loss = 0.5476555228233337\n",
      "[2021-11-04 01:48:59.476073] -- Validating: batch 100/157 from epoch 09/60 -- Loss = 0.5261803269386292\n",
      "[2021-11-04 01:49:03.205000] -- Validating: batch 150/157 from epoch 09/60 -- Loss = 0.3973487615585327\n",
      "[2021-11-04 01:49:03.732939] -- Validating: batch 157/157 from epoch 09/60 -- Loss = 0.5920120477676392\n",
      "[2021-11-04 01:49:03.733936] -- Epoch 09/60 -- Train loss = 0.5112530861418849, Validation loss = 0.5096392445503526 \n",
      "[2021-11-04 01:49:03.733936] -- Best loss 0.5082761037881207\n",
      "[2021-11-04 01:49:03.733936] -- Current lr = [0.0005]\n",
      "[2021-11-04 01:49:35.360191] -- Training:   batch 150/626 from epoch 10/60 -- Loss = 0.48318085074424744\n",
      "[2021-11-04 01:50:07.240146] -- Training:   batch 300/626 from epoch 10/60 -- Loss = 0.546916127204895\n",
      "[2021-11-04 01:50:39.066655] -- Training:   batch 450/626 from epoch 10/60 -- Loss = 0.5671422481536865\n",
      "[2021-11-04 01:51:10.680893] -- Training:   batch 600/626 from epoch 10/60 -- Loss = 0.46490198373794556\n",
      "[2021-11-04 01:51:15.991044] -- Training:   batch 626/626 from epoch 10/60 -- Loss = 1.0660936832427979\n",
      "[2021-11-04 01:51:19.669518] -- Validating: batch 050/157 from epoch 10/60 -- Loss = 0.6029476523399353\n",
      "[2021-11-04 01:51:23.332450] -- Validating: batch 100/157 from epoch 10/60 -- Loss = 0.5060207843780518\n",
      "[2021-11-04 01:51:27.042058] -- Validating: batch 150/157 from epoch 10/60 -- Loss = 0.486637681722641\n",
      "[2021-11-04 01:51:27.540777] -- Validating: batch 157/157 from epoch 10/60 -- Loss = 0.5848506689071655\n",
      "[2021-11-04 01:51:27.540777] -- Epoch 10/60 -- Train loss = 0.5105284475289975, Validation loss = 0.5104822263975811 \n",
      "[2021-11-04 01:51:27.540777] -- Best loss 0.5082761037881207\n",
      "[2021-11-04 01:51:27.540777] -- Current lr = [0.00035]\n",
      "[2021-11-04 01:51:59.054927] -- Training:   batch 150/626 from epoch 11/60 -- Loss = 0.526344895362854\n",
      "[2021-11-04 01:52:30.405544] -- Training:   batch 300/626 from epoch 11/60 -- Loss = 0.481927752494812\n",
      "[2021-11-04 01:53:02.024638] -- Training:   batch 450/626 from epoch 11/60 -- Loss = 0.46209049224853516\n",
      "[2021-11-04 01:53:33.366237] -- Training:   batch 600/626 from epoch 11/60 -- Loss = 0.5964540839195251\n",
      "[2021-11-04 01:53:38.625936] -- Training:   batch 626/626 from epoch 11/60 -- Loss = 1.6251957416534424\n",
      "[2021-11-04 01:53:42.322082] -- Validating: batch 050/157 from epoch 11/60 -- Loss = 0.605717658996582\n",
      "[2021-11-04 01:53:46.052996] -- Validating: batch 100/157 from epoch 11/60 -- Loss = 0.5459702014923096\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "scheduler = StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)\n",
    "model_manager = ModelManager(MODEL_NAME, make_valid_path(MODELS_DIR, is_dir=True, exist_ok=True))\n",
    "model, model_manager = train(net, model_manager, train_loader, val_loader, N_EPOCHS, loss_fn, optimizer, scheduler,\n",
    "                             get_accelerator('cuda'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump model manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(MODELS_DIR, MODEL_NAME+'.mgr'), 'wb') as handler:\n",
    "    pickle.dump(model_manager, handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_losses(\"Losses\", model_manager.epochs_losses_train, model_manager.epochs_losses_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model_manager.last_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used saved model to ensure that weights were successfully saved\n",
    "info(\"Validating model\")\n",
    "model = AntiSpoofingRNN(INPUT_SIZE, HIDDEN_SIZE, LSTM_NUM_LAYERS, LINEAR_SIZE, OUTPUT_SIZE)\n",
    "model_manager = ModelManager(MODEL_NAME, make_valid_path(MODELS_DIR, is_dir=True, exist_ok=True))\n",
    "model, _ = model_manager.load_checkpoint(MODEL_NAME + '.pkl', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find train metrics\n",
    "info(\"Validating performance on train data\")\n",
    "files, y_true, y_pred, y_prob, threshold = test(model, train_loader, pred_threshold='auto',\n",
    "                                                device=get_accelerator('cuda'))\n",
    "export_incorrect_samples_to_csv(files, y_pred, y_true, os.path.join(OUTPUT_DIR, 'train_incorrect.csv'))\n",
    "\n",
    "info(f\"Classification threshold = {threshold}\")\n",
    "info(\"Calculating train metrics\")\n",
    "train_metrics, train_metrics_str = Metrics.calculate_metrics(y_true, y_pred, y_prob)\n",
    "info(f\"Train accuracy = {train_metrics['Accuracy']:0.4f}\")\n",
    "\n",
    "# Second, find validation metrics\n",
    "info(\"Validating performance on validation data\")\n",
    "val_dataset = ReplySpoofDataset(VAL_INDEX)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_pad)\n",
    "files, y_true, y_pred, y_prob, threshold = test(model, val_loader, pred_threshold=threshold,\n",
    "                                                device=get_accelerator('cuda'))\n",
    "\n",
    "export_incorrect_samples_to_csv(files, y_pred, y_true, os.path.join(OUTPUT_DIR, 'val_incorrect.csv'))\n",
    "\n",
    "info(\"Calculating validation metrics\")\n",
    "val_metrics, val_metrics_str = Metrics.calculate_metrics(y_true, y_pred, y_prob)\n",
    "info(f\"Validation accuracy = {val_metrics['Accuracy']:0.4f}\")\n",
    "\n",
    "# Save metrics to file\n",
    "with open(os.path.join(OUTPUT_DIR, 'metrics.csv'), 'w') as f:\n",
    "    train_metrics['#'] = 'Train'\n",
    "    val_metrics['#'] = 'Validation'\n",
    "    w = csv.DictWriter(f, sorted(val_metrics.keys()))\n",
    "    w.writeheader()\n",
    "    w.writerow(train_metrics)\n",
    "    w.writerow(val_metrics)\n",
    "\n",
    "info(\"Validating model done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating labels of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCALER_PATH, \"rb\") as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = []\n",
    "test_preds = []\n",
    "for file_id, wav_file in enumerate(os.listdir(TEST_RAW_DIR)):\n",
    "    audio_path = os.path.join(TEST_RAW_DIR, wav_file)\n",
    "    out = test_sample(model, audio_path, scaler, pred_threshold=threshold) #0.2630763351917267\n",
    "    test_files.append(wav_file)\n",
    "    test_preds.append(out)\n",
    "    if (file_id + 1) % 500 ==0:\n",
    "        info(f\"{file_id+1:4d} file(s) processed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = os.path.join(OUTPUT_DIR, 'test_predictions.csv')\n",
    "pd.DataFrame(zip(test_files, test_preds)).to_csv(out_file, index=False, header=['Files', 'Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listen and Test some sample file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_audio = 'E:/Datasets/ID R&D/data/raw/Testing_Data/sample_0000.wav'\n",
    "ipd.Audio(sample_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_sample(model, sample_audio, scaler, return_code=False, pred_threshold=threshold)\n",
    "info(f\"The sample audio '{os.path.basename(sample_audio)}' is: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (ReplySpoofDetection)",
   "language": "python",
   "name": "pycharm-dc0b76cb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
